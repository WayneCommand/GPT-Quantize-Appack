name: Deploy Models

on:
  push:
    branches: [ "main" ]
    tags:
      - 'v0.*'
      - 'draft-*'

env:
  CARGO_TERM_COLOR: always
  AWS_EC2_METADATA_DISABLED: true

jobs:
  DeployModel:

    runs-on: self-hosted

    steps:
    - uses: actions/setup-python@v5.2.0
    - uses: actions/checkout@v4.1.7
    - name: set ENV
      run: echo "WORK_PATH=${GITHUB_WORKSPACE}/works" >> $GITHUB_ENV
    - name: make build path
      run: mkdir -p $WORK_PATH
    - name: build chatglm.cpp
      run: ./environment/chatglm.cpp.sh
    - name: build llama.cpp
      run: ./environment/llama.cpp.sh
    - name: Configure AWS Credentials
      run: |
        python3 -m pip install awscli --break-system-packages
        aws configure set aws_access_key_id ${{ secrets.AWS_KEY_ID }}
        aws configure set aws_secret_access_key ${{ secrets.AWS_KEY }}
    - name: pull model
      run: aws s3 cp s3://wayne/glm-4-9b-chat ${WORK_PATH}/GLM-4-9B-Chat --recursive --endpoint-url=https://s3.tebi.io --only-show-errors
    - name: pwd
      run: pwd && ls -lha
    - name: quantize llama
      run: ./chatglm/quantize-mode-llama.sh
    - name: quantize chatglm
      run: ./chatglm/quantize-model-chatglm.sh
